{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1zJxFiFWwf4RU2IqmY-Nf-55101wOcsVf","authorship_tag":"ABX9TyOgBauu9JQsLHkesBAVfO5P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XW3lIAp2oX5g","executionInfo":{"status":"ok","timestamp":1746535561286,"user_tz":-480,"elapsed":12104,"user":{"displayName":"鄭允揚","userId":"05387191466755258365"}},"outputId":"9088e297-e60e-468f-d847-b8b367c73100"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import math\n","import csv\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.metrics import roc_auc_score\n","\n","def FFT(xreal, ximag):\n","    n = 2\n","    while(n*2 <= len(xreal)):\n","        n *= 2\n","\n","    p = int(math.log(n, 2))\n","\n","    for i in range(0, n):\n","        a = i\n","        b = 0\n","        for j in range(0, p):\n","            b = int(b*2 + a%2)\n","            a = a/2\n","        if(b > i):\n","            xreal[i], xreal[b] = xreal[b], xreal[i]\n","            ximag[i], ximag[b] = ximag[b], ximag[i]\n","\n","    wreal = []\n","    wimag = []\n","\n","    arg = float(-2 * math.pi / n)\n","    treal = float(math.cos(arg))\n","    timag = float(math.sin(arg))\n","\n","    wreal.append(float(1.0))\n","    wimag.append(float(0.0))\n","\n","    for j in range(1, int(n/2)):\n","        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n","        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n","\n","    m = 2\n","    while(m < n + 1):\n","        for k in range(0, n, m):\n","            for j in range(0, int(m/2), 1):\n","                index1 = k + j\n","                index2 = int(index1 + m / 2)\n","                t = int(n * j / m)\n","                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n","                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n","                ureal = xreal[index1]\n","                uimag = ximag[index1]\n","                xreal[index1] = ureal + treal\n","                ximag[index1] = uimag + timag\n","                xreal[index2] = ureal - treal\n","                ximag[index2] = uimag - timag\n","        m *= 2\n","\n","    return n, xreal, ximag\n","\n","def FFT_data(input_data, swinging_times):\n","    txtlength = swinging_times[-1] - swinging_times[0]\n","    a_mean = [0] * txtlength\n","    g_mean = [0] * txtlength\n","\n","    for num in range(len(swinging_times)-1):\n","        a = []\n","        g = []\n","        for swing in range(swinging_times[num], swinging_times[num+1]):\n","            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n","            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n","\n","        a_mean[num] = (sum(a) / len(a))\n","        g_mean[num] = (sum(a) / len(a)) # Fixed: using a instead of g to calculate g_mean\n","\n","    return a_mean, g_mean\n","\n","def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n","    allsum = []\n","    mean = []\n","    var = []\n","    rms = []\n","    XYZmean_a = 0\n","    a = []\n","    g = []\n","    a_s1 = 0\n","    a_s2 = 0\n","    g_s1 = 0\n","    g_s2 = 0\n","    a_k1 = 0\n","    a_k2 = 0\n","    g_k1 = 0\n","    g_k2 = 0\n","\n","    for i in range(len(input_data)):\n","        if i==0:\n","            allsum = input_data[i]\n","            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n","            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n","            continue\n","\n","        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n","        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n","\n","        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n","\n","    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n","\n","    for i in range(len(input_data)):\n","        if i==0:\n","            var = input_data[i]\n","            rms = input_data[i]\n","            continue\n","\n","        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n","        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n","\n","    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n","    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n","\n","    a_max = [max(a)]\n","    a_min = [min(a)]\n","    a_mean = [sum(a) / len(a)]\n","    g_max = [max(g)]\n","    g_min = [min(g)]\n","    g_mean = [sum(g) / len(g)]\n","\n","    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n","\n","    for i in range(len(input_data)):\n","        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n","        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n","        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n","        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n","        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n","        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n","\n","    a_s1 = a_s1 / len(input_data)\n","    a_s2 = a_s2 / len(input_data)\n","    g_s1 = g_s1 / len(input_data)\n","    g_s2 = g_s2 / len(input_data)\n","    a_k2 = math.pow(a_s2, 1.5)\n","    g_k2 = math.pow(g_s2, 1.5)\n","    a_s2 = a_s2 * a_s2\n","    g_s2 = g_s2 * g_s2\n","\n","    a_kurtosis = [a_s1 / a_s2]\n","    g_kurtosis = [g_s1 / g_s2]\n","    a_skewness = [a_k1 / a_k2]\n","    g_skewness = [g_k1 / g_k2]\n","\n","    a_fft_mean = 0\n","    g_fft_mean = 0\n","    cut = int(n_fft / swinging_times)\n","    a_psd = []\n","    g_psd = []\n","    entropy_a = []\n","    entropy_g = []\n","    e1 = []\n","    e3 = []\n","    e2 = 0\n","    e4 = 0\n","\n","    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n","        a_fft_mean += a_fft[i]\n","        g_fft_mean += g_fft[i]\n","        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n","        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n","        e1.append(math.pow(a_psd[-1], 0.5))\n","        e3.append(math.pow(g_psd[-1], 0.5))\n","\n","    a_fft_mean = a_fft_mean / cut\n","    g_fft_mean = g_fft_mean / cut\n","\n","    a_psd_mean = sum(a_psd) / len(a_psd)\n","    g_psd_mean = sum(g_psd) / len(g_psd)\n","\n","    for i in range(cut):\n","        e2 += math.pow(a_psd[i], 0.5)\n","        e4 += math.pow(g_psd[i], 0.5)\n","\n","    for i in range(cut):\n","        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n","        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n","\n","    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n","    g_entropy_mean = sum(entropy_g) / len(entropy_g)\n","\n","\n","    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n","    writer.writerow(output)\n","\n","\n","def data_generate():\n","    base_path = '/content/drive/MyDrive/AI_CUP'  # Define the base path\n","    datapath = Path(base_path) / 'train_data'  # Path to training data\n","    tar_dir = Path(base_path) / 'tabular_data_train'  # Path to output directory\n","    pathlist_txt = Path(datapath).glob('**/*.txt')\n","\n","    for file in pathlist_txt:\n","        f = open(file)\n","        All_data = []\n","        count = 0\n","        for line in f.readlines():\n","            if line == '\\n' or count == 0:\n","                count += 1\n","                continue\n","            num = line.split(' ')\n","            if len(num) > 5:\n","                tmp_list = []\n","                for i in range(6):\n","                    tmp_list.append(int(num[i]))\n","                All_data.append(tmp_list)\n","\n","        f.close()\n","\n","        swing_index = np.linspace(0, len(All_data), 28, dtype = int)\n","\n","        headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']\n","\n","        with open(tar_dir / f'{Path(file).stem}.csv', 'w', newline = '') as csvfile:\n","            writer = csv.writer(csvfile)\n","            writer.writerow(headerList)\n","            try:\n","                a_fft, g_fft = FFT_data(All_data, swing_index)\n","                a_fft_imag = [0] * len(a_fft)\n","                g_fft_imag = [0] * len(g_fft)\n","                n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n","                n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n","                for i in range(len(swing_index)):\n","                    if i==0:\n","                        continue\n","                    feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n","            except Exception as e:  # Print the actual error message for debugging\n","                print(f\"Error processing file {Path(file).stem}: {e}\")\n","                continue\n","\n","\n","def main():\n","    base_path = '/content/drive/MyDrive/AI_CUP'  # Define the base path\n","    # data_generate()  # Uncomment to generate feature CSV files\n","\n","    # Read training info using the base path\n","    info = pd.read_csv(Path(base_path) / 'train_info.csv')\n","    unique_players = info['player_id'].unique()\n","    train_players, test_players = train_test_split(unique_players, test_size=0.2, random_state=42)\n","\n","    # Read feature CSV files using the base path\n","    datapath = Path(base_path) / 'tabular_data_train'\n","    datalist = list(datapath.glob('**/*.csv'))\n","    target_mask = ['gender', 'hold racket handed', 'play years', 'level']\n","\n","    x_train = pd.DataFrame()\n","    y_train = pd.DataFrame(columns=target_mask)\n","    x_test = pd.DataFrame()\n","    y_test = pd.DataFrame(columns=target_mask)\n","\n","    for file in datalist:\n","        unique_id = int(Path(file).stem)\n","        row = info[info['unique_id'] == unique_id]\n","\n","        # Check if the row is empty and print a message if it is\n","        if row.empty:\n","            print(f\"Warning: No matching info found for unique_id {unique_id}\")\n","            continue\n","\n","        player_id = row['player_id'].iloc[0]\n","\n","        try:  # Wrap data loading in a try-except block\n","            data = pd.read_csv(file)\n","        except pd.errors.EmptyDataError:  # Handle empty CSV files\n","            print(f\"Warning: CSV file {file} is empty. Skipping...\")\n","            continue\n","        except Exception as e:  # Handle other potential errors during loading\n","            print(f\"Error reading CSV file {file}: {e}\")\n","            continue\n","\n","        target = row[target_mask]\n","        target_repeated = pd.concat([target] * len(data))\n","        if player_id in train_players:\n","            x_train = pd.concat([x_train, data], ignore_index=True)\n","            y_train = pd.concat([y_train, target_repeated], ignore_index=True)\n","        elif player_id in test_players:\n","            x_test = pd.concat([x_test, data], ignore_index=True)\n","            y_test = pd.concat([y_test, target_repeated], ignore_index=True)\n","\n","    # Check if x_train is still empty after loading data\n","    if x_train.empty:\n","        print(\"Error: x_train is empty. Check data loading or file paths.\")\n","        return  # Exit the function early if x_train is empty\n","\n","    scaler = MinMaxScaler()\n","    le = LabelEncoder()\n","    X_train_scaled = scaler.fit_transform(x_train)\n","    X_test_scaled = scaler.transform(x_test)\n","\n","    group_size = 27\n","\n","    def model_binary(X_train, y_train, X_test, y_test):\n","        clf = RandomForestClassifier(random_state=42)\n","        clf.fit(X_train, y_train)\n","\n","        predicted = clf.predict_proba(X_test)\n","        predicted = [predicted[i][0] for i in range(len(predicted))]\n","\n","        num_groups = len(predicted) // group_size\n","        if sum(predicted[:group_size]) / group_size > 0.5:\n","            y_pred = [max(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n","        else:\n","            y_pred = [min(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n","\n","        y_pred  = [1 - x for x in y_pred]\n","        y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n","\n","        auc_score = roc_auc_score(y_test_agg, y_pred, average='micro')\n","        print(auc_score)\n","\n","    def model_multiary(X_train, y_train, X_test, y_test):\n","        clf = RandomForestClassifier(random_state=42)\n","        clf.fit(X_train, y_train)\n","        predicted = clf.predict_proba(X_test)\n","        num_groups = len(predicted) // group_size\n","        y_pred = []\n","        for i in range(num_groups):\n","            group_pred = predicted[i*group_size: (i+1)*group_size]\n","            num_classes = len(np.unique(y_train))\n","            class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n","            chosen_class = np.argmax(class_sums)\n","            candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n","            best_instance = np.argmax(candidate_probs)\n","            y_pred.append(group_pred[best_instance])\n","\n","        y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n","        auc_score = roc_auc_score(y_test_agg, y_pred, average='micro', multi_class='ovr')\n","        print('Multiary AUC:', auc_score)\n","\n","    y_train_le_gender = le.fit_transform(y_train['gender'])\n","    y_test_le_gender = le.transform(y_test['gender'])\n","    model_binary(X_train_scaled, y_train_le_gender, X_test_scaled, y_test_le_gender)\n","\n","    y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n","    y_test_le_hold = le.transform(y_test['hold racket handed'])\n","    model_binary(X_train_scaled, y_train_le_hold, X_test_scaled, y_test_le_hold)\n","\n","    y_train_le_years = le.fit_transform(y_train['play years'])\n","    y_test_le_years = le.transform(y_test['play years'])\n","    model_multiary(X_train_scaled, y_train_le_years, X_test_scaled, y_test_le_years)\n","\n","    y_train_le_level = le.fit_transform(y_train['level'])\n","    y_test_le_level = le.transform(y_test['level'])\n","    model_multiary(X_train_scaled, y_train_le_level, X_test_scaled, y_test_le_level)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5SRb2fHrUOq","executionInfo":{"status":"ok","timestamp":1746536583609,"user_tz":-480,"elapsed":81,"user":{"displayName":"鄭允揚","userId":"05387191466755258365"}},"outputId":"0c5613a6-d33d-4ad3-fcd0-d8bf24e8cbbb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: x_train is empty. Check data loading or file paths.\n"]}]}]}